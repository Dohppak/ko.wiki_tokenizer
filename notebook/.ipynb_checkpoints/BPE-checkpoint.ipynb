{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Ko-Wiki BPE\n",
    "\n",
    "- https://github.com/aisolab/nlp_classification/blob/master/Convolutional_Neural_Networks_for_Sentence_Classification/model/utils.py\n",
    "\n",
    "- 본 문서는 위 레파지토리를 참고하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, ByteLevelBPETokenizer\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    join(\"../tokenize_model\", \"{}-vocab.json\".format(\"bpe-bytelevel\")),\n",
    "    join(\"../tokenize_model\", \"{}-merges.txt\".format(\"bpe-bytelevel\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'P',\n",
       " 'E',\n",
       " 'ë¥¼',\n",
       " 'ĠíķľêµŃìĸ´',\n",
       " 'ĠìľĦíĤ¤',\n",
       " 'íĶ¼ë',\n",
       " 'ĶĶìķĦ',\n",
       " 'Ġëį°ìĿ´íĦ°',\n",
       " 'ìħĭ',\n",
       " 'ìĿĦ',\n",
       " 'ĠíĻľìļ©',\n",
       " 'íķĺìĹ¬',\n",
       " 'ĠíĨł',\n",
       " 'íģ¬',\n",
       " 'ëĤĺìĿ´',\n",
       " 'ì§ķ',\n",
       " 'íķ´ë',\n",
       " '´',\n",
       " '¤',\n",
       " 'ìĬµëĭĪëĭ¤',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"BPE를 한국어 위키피디아 데이터셋을 활용하여 토크나이징해봤습니다.\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv(\"./nsmc/train.txt\", sep=\"\\t\").loc[:, [\"document\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>애들 욕하지마라 지들은 뭐 그렇게 잘났나? 솔까 거기 나오는 귀여운 애들이 당신들보...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>여전히 반복되고 있는 80년대 한국 멜로 영화의 유치함.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>쉐임리스 스티브와 피오나가 손오공 부르마로 ㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0점은 없나요?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>제발 시즌2 ㅜㅜ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  label\n",
       "0  애들 욕하지마라 지들은 뭐 그렇게 잘났나? 솔까 거기 나오는 귀여운 애들이 당신들보...      1\n",
       "1                    여전히 반복되고 있는 80년대 한국 멜로 영화의 유치함.      0\n",
       "2                        쉐임리스 스티브와 피오나가 손오공 부르마로 ㅋㅋㅋ      0\n",
       "3                                        0점은 없나요?...      0\n",
       "4                                          제발 시즌2 ㅜㅜ      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [ìķł, ëĵ¤, Ġìļķ, íķĺì§Ģ, ë§Ī, ëĿ¼, Ġì§Ģ, ëĵ¤ìĿ...\n",
       "1    [ìĹ¬, ìłĦíŀĪ, Ġë°ĺë³µ, ëĲĺê³ł, ĠìŀĪëĬĶ, Ġ, 8, ...\n",
       "2    [ìī, Ĳ, ìŀ, Ħë¦¬, ìĬ¤, ĠìĬ¤íĭ°, ë¸Į, ìĻĢ, ĠíĶ¼...\n",
       "3              [0, ìłĲìĿĢ, ĠìĹĨ, ëĤĺ, ìļĶ, ?, ., ., .]\n",
       "4            [ìłľ, ë°ľ, Ġìĭľì¦Į, 2, Ġã, ħ, ľ, ã, ħ, ľ]\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_doc = tr[\"document\"].apply(tokenizer.encode).map(lambda x: x.tokens)\n",
    "bpe_doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make tokenizer class\n",
    "\n",
    "from typing import List, Callable, Union, Dict\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizer class from gluonnlp\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        \"\"\"\n",
    "        To do: Make your vocab class\n",
    "        \"\"\"\n",
    "        vocab: None,\n",
    "        split_fn: Callable[[str], List[str]],\n",
    "        pad_fn: Callable[[List[int]], List[int]] = None,\n",
    "    ) -> None:\n",
    "        self._vocab = vocab\n",
    "        self._split = split_fn\n",
    "        self._pad = pad_fn\n",
    "\n",
    "    def split(self, string: str) -> List[str]:\n",
    "        list_of_tokens = self._split(string).tokens\n",
    "        return list_of_tokens\n",
    "\n",
    "    def transform(self, list_of_tokens: List[str]) -> List[int]:\n",
    "        list_of_indices = self._vocab.to_indices(list_of_tokens)\n",
    "        list_of_indices = self._pad(list_of_indices) if self._pad else list_of_indices\n",
    "        return list_of_indices\n",
    "\n",
    "    def split_and_transform(self, string: str) -> List[int]:\n",
    "        return self.transform(self.split(string).tokens)\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self._vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab Class를 구현하고자 한다면 aisolab repo를 확인하세요!\n",
    "tokenizer = Tokenizer(vocab=None, split_fn=tokenizer.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
